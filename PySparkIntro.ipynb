{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86168395-c2ef-49a3-9e26-c3f5bb4e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad45b8c-7eb3-4a37-8234-8568a284a039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "054521e7-88ea-41c1-a18f-fd410c6df88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e5088a-d6ad-40c9-bf52-567dc0b9f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b04b3b-7582-47b6-aeb9-3da117c53669",
   "metadata": {},
   "source": [
    "The SparkSession is the unified single entry point and the gateway to all Spark functionality and data in your PySpark application. It's the very first object you create to interact with Spark, and you use it to read data, create DataFrames, execute SQL queries, and access all configuration settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32b210-5775-4f1e-84ac-b8e7117a6289",
   "metadata": {},
   "source": [
    "builder: The starting point for constructing the session.\n",
    "\n",
    "appName(name): Gives a name to your application, which is displayed on the Spark cluster web UI.\n",
    "\n",
    "config(key, value): Sets various Spark configuration parameters.\n",
    "\n",
    "master(url): Specifies the Spark cluster URL (e.g., local, local[4], spark://host:port). Note: This is often set via command-line arguments or cluster manager settings, so it's less common in code for production.\n",
    "\n",
    "getOrCreate(): This is crucial. It tries to get an existing SparkSession if one is already running (e.g., in a notebook environment); otherwise, it creates a new one. This prevents the creation of multiple sessions in the same JVM.\n",
    "\n",
    "spark.stop(): Terminates the SparkSession and releases all resources. Always call this at the end of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af93450f-7900-43ba-934b-a77d1e48f37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2bf6c83b5c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a272f5a3-2bfc-4500-b897-5388bfb55194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc88284-49f6-4424-b96b-e41a29005368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93f2873f-4775-466f-ab1f-5c5e260fc4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      _c0|_c1|       _c2|   _c3|\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d97028e-5970-4361-b433-c40719ce7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d362c87b-d6be-4d9e-943c-274cf43759b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "701287e5-c689-49f3-962f-d46d75ef39ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e33b57d-e56e-4257-9a2a-4fe2e8ffb03d",
   "metadata": {},
   "source": [
    "#example\n",
    " import tempfile\n",
    ">>> with tempfile.TemporaryDirectory(prefix=\"option\") as d:\n",
    "...     # Write a DataFrame into a CSV file\n",
    "...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])\n",
    "...     df.write.mode(\"overwrite\").format(\"csv\").save(d)\n",
    "...\n",
    "...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.\n",
    "...     spark.read.schema(df.schema).option(\n",
    "...         \"nullValue\", \"Hyukjin Kwon\").format('csv').load(d).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b2698f6-5005-4fbd-9714-08c5727dc0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()#like df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ce350d6-46d9-437e-94d6-60c1a3e5202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('test1.csv',inferSchema=True)# if u don't give inferSchema=True then it will consider by default all the features as strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2ce611b-8c7a-4f23-8660-293912e719da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
